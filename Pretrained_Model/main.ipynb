{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. 데이터 로드\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ppr_data(df):\n",
    "    # TODO: 데이터 전처리 코드 구현 ---------- #\n",
    "    df = df.groupby('ticketno').apply(custom_info).reset_index()  # ticketno 기준 병합 및 전처리 수행\n",
    "\n",
    "    x_df = df.iloc[:, :-1]\n",
    "    y_df = df['root_cause_type']\n",
    "    \n",
    "    y_df.value_counts().plot(kind = 'bar') if y_df[0] is not None else None  # 레이블 분포 확인\n",
    "    \n",
    "    # ------------------------------------- #\n",
    "    return x_df, y_df\n",
    "\n",
    "def custom_info(group):  # 임의로 작성된 전처리 코드입니다.\n",
    "    d = {}\n",
    "    group.sort_values(by='alarmtime', ascending=True, inplace=True)  # 경보 순서 정렬\n",
    "    d['alarmmsg_original'] = ' '.join(group['alarmmsg_original'])  # 메시지 단순 병합\n",
    "    if 'root_cause_type' in group.columns:  # 레이블 추출\n",
    "        group['root_cause_type'] = group['root_cause_type'].apply(lambda x: 0 if x == 'PowerFail' else (1 if x == 'LinkCut' else 2))  # 레이블 인코딩\n",
    "        d['root_cause_type'] = group['root_cause_type'].iloc[0]  # 동일한 ticketno는 동일한 root_cause_type을 가짐\n",
    "    else:\n",
    "        d['root_cause_type'] = None  # 테스트 세트의 경우 정답 컬럼 없음\n",
    "    return pd.Series(d, index=['alarmmsg_original', 'root_cause_type'])\n",
    "\n",
    "train_df = pd.read_csv(\"Q2_train.csv\")\n",
    "test_df = pd.read_csv(\"Q2_test.csv\")\n",
    "\n",
    "x_train_df, y_train_df = ppr_data(train_df)\n",
    "x_test_df, _ = ppr_data(test_df)  # 테스트 세트의 경우 정답 컬럼 없음\n",
    "\n",
    "# print(x_train_df,'\\n', y_train_df)\n",
    "# print(x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\ssjj3\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  tensor([[    0,  3888,   791,  ...,     1,     1,     1],\n",
      "        [    0, 31076,    12,  ...,     1,     1,     1],\n",
      "        [    0,  3888,   791,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0,  5733,   565,  ...,     1,     1,     1],\n",
      "        [    0,  5733,   565,  ...,     1,     1,     1],\n",
      "        [    0,  5733,   565,  ...,     1,     1,     1]])  /  torch.Size([1114, 256])\n",
      "attention_masks :  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])  /  torch.Size([1114, 256])\n",
      "labels :  tensor([0, 0, 0,  ..., 1, 1, 1])  /  torch.Size([1114])\n",
      "------------------------------ tokenizing end ------------------------------\n",
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "  Average training loss: 0.2009\n",
      "  Training epcoh took: 00:04:22\n",
      "======== Epoch 2 / 10 ========\n",
      "  Average training loss: 0.0665\n",
      "  Training epcoh took: 00:08:41\n",
      "======== Epoch 3 / 10 ========\n",
      "  Average training loss: 0.0395\n",
      "  Training epcoh took: 00:13:33\n",
      "======== Epoch 4 / 10 ========\n",
      "  Average training loss: 0.0425\n",
      "  Training epcoh took: 00:18:16\n",
      "======== Epoch 5 / 10 ========\n",
      "  Average training loss: 0.0357\n",
      "  Training epcoh took: 00:22:55\n",
      "======== Epoch 6 / 10 ========\n",
      "  Average training loss: 0.0262\n",
      "  Training epcoh took: 00:27:35\n",
      "======== Epoch 7 / 10 ========\n",
      "  Average training loss: 0.0182\n",
      "  Training epcoh took: 00:32:13\n",
      "======== Epoch 8 / 10 ========\n",
      "  Average training loss: 0.0277\n",
      "  Training epcoh took: 00:36:51\n",
      "======== Epoch 9 / 10 ========\n",
      "  Average training loss: 0.0278\n",
      "  Training epcoh took: 00:41:29\n",
      "======== Epoch 10 / 10 ========\n",
      "  Average training loss: 0.0241\n",
      "  Training epcoh took: 00:46:08\n",
      "------------------------------ train end ------------------------------\n",
      "\n",
      "['PowerFail' 'PowerFail' 'PowerFail' ... 'PowerFail' 'PowerFail'\n",
      " 'PowerFail']\n",
      "------------------------------ predict end ------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoTokenizer, AutoModel, AutoModelForSequenceClassification, TFAutoModelForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)\n",
    "\n",
    "# 2. 모델 학습 및 예측\n",
    "class MyModel:\n",
    "    def __init__(self) -> None:\n",
    "        self.model = model\n",
    "        self.max_len = 256 # 문장 최대 길이\n",
    "        self.epoch = 10 # 학습 횟수\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        #x_ticketno = x_train['ticketno'].values\n",
    "        x_alarmmsg_original = x_train['alarmmsg_original'].values\n",
    "        y_root_cause_type = y_train.values\n",
    "\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        #token_type_ids = []\n",
    "\n",
    "        for i, message in enumerate(x_alarmmsg_original):\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                                message,                       # 문장\n",
    "                                add_special_tokens = True,     # 문장 맨 앞, 뒤에 [CLS], [SEP] 추가\n",
    "                                max_length = self.max_len,     # 문장 최대 길이\n",
    "                                pad_to_max_length = True,      # 문장 길이가 짧으면 패딩 추가\n",
    "                                return_attention_mask = True,  # 어텐션 마스크 생성\n",
    "                                return_tensors = 'pt',         # 파이토치 텐서 반환\n",
    "                           )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # 리스트를 텐서로 변환\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        labels = torch.tensor(y_root_cause_type)\n",
    "        print('input_ids : ', input_ids, ' / ', input_ids.shape)\n",
    "        print('attention_masks : ', attention_masks, ' / ', attention_masks.shape)\n",
    "        print('labels : ', labels, ' / ', labels.shape)\n",
    "\n",
    "        print('------------------------------ tokenizing end ------------------------------\\n')\n",
    "\n",
    "        # 모델 학습\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-5)  # AdamW 옵티마이저 사용, 학습률은 2e-5\n",
    "\n",
    "        # 데이터 로더 설정\n",
    "        dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "        train_sampler = RandomSampler(dataset)\n",
    "        train_dataloader = DataLoader(dataset, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "        # 학습 반복\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        t0 = time.time()\n",
    "        format_time = lambda elapsed: time.strftime(\"%H:%M:%S\", time.gmtime(elapsed))\n",
    "\n",
    "        for epoch_i in range(self.epoch):\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, self.epoch))\n",
    "            total_loss = 0\n",
    "            self.model.train()\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                if step % 100 == 0 and not step == 0:\n",
    "                    elapsed = format_time(time.time() - t0)\n",
    "                    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            avg_loss = total_loss / len(train_dataloader)\n",
    "            print('  Average training loss: {0:.4f}'.format(avg_loss))\n",
    "            print('  Training epcoh took: {:}'.format(format_time(time.time() - t0)))\n",
    "\n",
    "        # 모델 저장\n",
    "        self.model.save_pretrained(\"saved_model\")\n",
    "        tokenizer.save_pretrained(\"saved_model\")\n",
    "        torch.save(self.model, 'saved_model/model.pt')\n",
    "        \n",
    "        print('------------------------------ train end ------------------------------\\n')\n",
    "\n",
    "        pass\n",
    "\n",
    "    def predict(self, x_test):\n",
    "        x_alarmmsg_original = x_test['alarmmsg_original'].values\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "\n",
    "        for i, message in enumerate(x_alarmmsg_original):\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                                message,                       # 문장\n",
    "                                add_special_tokens = True,     # 문장 맨 앞, 뒤에 [CLS], [SEP] 추가\n",
    "                                max_length = self.max_len,     # 문장 최대 길이\n",
    "                                pad_to_max_length = True,      # 문장 길이가 짧으면 패딩 추가\n",
    "                                return_attention_mask = True,  # 어텐션 마스크 생성\n",
    "                                return_tensors = 'pt',         # 파이토치 텐서 반환\n",
    "                           )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "        # 리스트를 텐서로 변환\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "        # 데이터 로더 설정\n",
    "        dataset = TensorDataset(input_ids, attention_masks)\n",
    "        test_sampler = SequentialSampler(dataset)\n",
    "        test_dataloader = DataLoader(dataset, sampler=test_sampler, batch_size=16)\n",
    "\n",
    "        # 예측\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "        pred = []\n",
    "        for batch in test_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred.append(logits)\n",
    "        pred = np.concatenate(pred, axis=0)\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        pred = np.where(pred == 0, 'PowerFail', np.where(pred == 1, 'LinkCut', 'UnitFail'))\n",
    "        print(pred)\n",
    "\n",
    "        pred_df = pd.DataFrame({'ticketno': x_test['ticketno'].values, 'root_cause_type': pred})\n",
    "\n",
    "        print('------------------------------ predict end ------------------------------\\n')\n",
    "\n",
    "        return pred_df\n",
    "\n",
    "model = MyModel()\n",
    "model.train(x_train_df, y_train_df)\n",
    "y_pred = model.predict(x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check: ticketno 순서와 샘플 수가 일치합니다.\n",
      "Done : Q2_submitResult.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 3. 결과 제출\n",
    "# 본 코드는 제출되는 파일의 형태에 대한 가이드로, 반드시 아래 구조를 따를 필요 없이 자유롭게 코드를 작성해도 무방합니다.\n",
    "# 제출 포맷에 대해선 data/Q2_label_sample.csv를 참조하세요.\n",
    "#\n",
    "# 분야 2의 경우, 전표(ticket) 하나에 하나의 근원장애(root_cause_type)을 매칭해야 합니다.\n",
    "#   주의: 경보(alarm) 개수와 전표(ticket) 개수는 다르며, 예측할 대상은 전표입니다.\n",
    "#   주의: ticketno 컬럼 기준으로 오름차순 정렬이 필요합니다.\n",
    "# 분야 2의 제출 파일은 2개 컬럼 [ticketno, root_cause_type]을 가져야 합니다.\n",
    "\n",
    "def submitResult(pred):\n",
    "    try:\n",
    "        label = pd.read_csv('Q2_label_sample.csv')\n",
    "        # ticketno 순서와 개수가 일치하는지 확인\n",
    "        if (label['ticketno'] == pred['ticketno']).all():\n",
    "            print(\"Check: ticketno 순서와 샘플 수가 일치합니다.\")\n",
    "        else:\n",
    "            print(\"Warning: 테스트 세트와 모델 예측의 ticketno가 일치하지 않습니다.\")\n",
    "            return\n",
    "\n",
    "        pred.to_csv('Q2_submitResult.csv', index=False)\n",
    "        print(\"Done : Q2_submitResult.csv 파일로 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        # 예외가 발생한 경우 오류 메시지 출력\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "submitResult(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU reset\n",
    "\n",
    "from numba import cuda\n",
    "\n",
    "device = cuda.get_current_device(); device.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
